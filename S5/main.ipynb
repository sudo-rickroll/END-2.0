{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Sentiment_Analysis_using_LSTM_RNN 2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S318CunI24Kj"
      },
      "source": [
        "# Data Pre-Processing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HI1nLG-CLaRz"
      },
      "source": [
        "## Installing and Importing the required libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FqH8vMqTpuca",
        "outputId": "a489d494-ca06-4d45-b957-63f22e6bd26a"
      },
      "source": [
        "!pip install pytreebank"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pytreebank in /usr/local/lib/python3.7/dist-packages (0.2.7)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oicj4H1hcKz5",
        "outputId": "36e7f5b9-029b-438e-8bcc-db5f955a887f"
      },
      "source": [
        "!pip install numpy git+https://github.com/makcedward/nlpaug.git"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting git+https://github.com/makcedward/nlpaug.git\n",
            "  Cloning https://github.com/makcedward/nlpaug.git to /tmp/pip-req-build-3p9j899t\n",
            "  Running command git clone -q https://github.com/makcedward/nlpaug.git /tmp/pip-req-build-3p9j899t\n",
            "Requirement already satisfied (use --upgrade to upgrade): nlpaug==1.1.3 from git+https://github.com/makcedward/nlpaug.git in /usr/local/lib/python3.7/dist-packages\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (1.19.5)\n",
            "Building wheels for collected packages: nlpaug\n",
            "  Building wheel for nlpaug (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for nlpaug: filename=nlpaug-1.1.3-cp37-none-any.whl size=837629 sha256=f328f764403df0c116888e580d6a5538a5999aca6667a51554264ab3afe419d0\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-bdzdhtvx/wheels/2b/ef/30/a4e22f9a97373c9ab6763670c94aa5e111b0b956983f3892a4\n",
            "Successfully built nlpaug\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I9ZJNA_KBsnH",
        "outputId": "c28730ae-f583-4401-c909-ab4204b1f9d0"
      },
      "source": [
        "! pip install googletrans==3.1.0a0"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: googletrans==3.1.0a0 in /usr/local/lib/python3.7/dist-packages (3.1.0a0)\n",
            "Requirement already satisfied: httpx==0.13.3 in /usr/local/lib/python3.7/dist-packages (from googletrans==3.1.0a0) (0.13.3)\n",
            "Requirement already satisfied: hstspreload in /usr/local/lib/python3.7/dist-packages (from httpx==0.13.3->googletrans==3.1.0a0) (2020.12.22)\n",
            "Requirement already satisfied: idna==2.* in /usr/local/lib/python3.7/dist-packages (from httpx==0.13.3->googletrans==3.1.0a0) (2.10)\n",
            "Requirement already satisfied: rfc3986<2,>=1.3 in /usr/local/lib/python3.7/dist-packages (from httpx==0.13.3->googletrans==3.1.0a0) (1.5.0)\n",
            "Requirement already satisfied: httpcore==0.9.* in /usr/local/lib/python3.7/dist-packages (from httpx==0.13.3->googletrans==3.1.0a0) (0.9.1)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.7/dist-packages (from httpx==0.13.3->googletrans==3.1.0a0) (1.2.0)\n",
            "Requirement already satisfied: chardet==3.* in /usr/local/lib/python3.7/dist-packages (from httpx==0.13.3->googletrans==3.1.0a0) (3.0.4)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from httpx==0.13.3->googletrans==3.1.0a0) (2020.12.5)\n",
            "Requirement already satisfied: h2==3.* in /usr/local/lib/python3.7/dist-packages (from httpcore==0.9.*->httpx==0.13.3->googletrans==3.1.0a0) (3.2.0)\n",
            "Requirement already satisfied: h11<0.10,>=0.8 in /usr/local/lib/python3.7/dist-packages (from httpcore==0.9.*->httpx==0.13.3->googletrans==3.1.0a0) (0.9.0)\n",
            "Requirement already satisfied: hyperframe<6,>=5.2.0 in /usr/local/lib/python3.7/dist-packages (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans==3.1.0a0) (5.2.0)\n",
            "Requirement already satisfied: hpack<4,>=3.0 in /usr/local/lib/python3.7/dist-packages (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans==3.1.0a0) (3.0.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qnj4UlH-qBvX"
      },
      "source": [
        "import pytreebank\n",
        "import pandas as pd\n",
        "import random\n",
        "import nlpaug.augmenter.word as naw\n",
        "import googletrans"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CtgHbgzvLho3"
      },
      "source": [
        "## Load Stanford Sentiment Analysis from the \"pytreebank\" library"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NaxbfTwEp_53"
      },
      "source": [
        "dataset = pytreebank.load_sst()"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zCskhn69s1Q1",
        "outputId": "665e36b4-06b8-4fb3-b7cd-df37e504670f"
      },
      "source": [
        "[dataset.keys()]"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[dict_keys(['train', 'test', 'dev'])]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dGa1aGf-8FoX",
        "outputId": "a7d30abe-d317-4e3e-a7e0-fda0bb232fc1"
      },
      "source": [
        "len(dataset['train']), len(dataset['test']), len(dataset['dev'])"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(8544, 2210, 1101)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aZ9I1vs6L-hC"
      },
      "source": [
        "## Prepare the Train, Validation and Test Dataset splits"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_iCSczZbuGLx"
      },
      "source": [
        "train_df = pd.DataFrame({'sentence' : [dataset['train'][i].to_labeled_lines()[0][1] for i in range(len(dataset['train']))], 'labels' : [dataset['train'][i].to_labeled_lines()[0][0] for i in range(len(dataset['train']))]})\n",
        "test_df = pd.DataFrame({'sentence' : [dataset['test'][i].to_labeled_lines()[0][1] for i in range(len(dataset['test']))], 'labels' : [dataset['test'][i].to_labeled_lines()[0][0] for i in range(len(dataset['test']))]})\n",
        "val_df = pd.DataFrame({'sentence' : [dataset['dev'][i].to_labeled_lines()[0][1] for i in range(len(dataset['dev']))], 'labels' : [dataset['dev'][i].to_labeled_lines()[0][0] for i in range(len(dataset['dev']))]})\n",
        "df = pd.concat([train_df, test_df, val_df]).reset_index(drop=True)\n",
        "df = df.sample(frac=1).reset_index(drop=True)\n"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rE-ylf6C6BJD",
        "outputId": "980b3816-8465-4da4-f530-8f16200396ed"
      },
      "source": [
        "df.shape"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(11855, 2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dkxhrQox6Ju3",
        "outputId": "75bbeb16-1ce4-447d-9b9e-93149b39386b"
      },
      "source": [
        "df.labels.value_counts()"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1    3140\n",
              "3    3111\n",
              "2    2242\n",
              "4    1852\n",
              "0    1510\n",
              "Name: labels, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1AgV_LXSLXjd"
      },
      "source": [
        "# Dataset split to train and validation. Percent of data to be included for training set. Rest will go to validation set\n",
        "train_split_pct = 70"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2reTNPc3v9RW"
      },
      "source": [
        "df_train = df[:(train_split_pct*len(df))//100]\n",
        "df_val = df[len(df_train):].reset_index(drop=True)"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ma2WESJtwP1A",
        "outputId": "773f76dc-b2c7-4c89-a2b6-c2437b4d5cee"
      },
      "source": [
        "len(df_train), len(df_val)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(8298, 3557)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OFwi4jEAMKfK"
      },
      "source": [
        "## Augment the Train Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZQNGHqolNRRk"
      },
      "source": [
        "# Percentage of training data to be augmented using random augmentations\n",
        "train_aug_pct = 5"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZYgC1u2VZEvq"
      },
      "source": [
        "indexes = random.choices(list(range(len(df_train))), k = (len(df_train)* train_aug_pct)//100)                         \n",
        "sentences = [df_train['sentence'][i] for i in indexes]\n",
        "augmented_sentences = []"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0xwq1H9-MSHn"
      },
      "source": [
        "### Random Augmentations from EDA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Au5IldDgN3Kt"
      },
      "source": [
        "# random augmentations to be done and the percentage of words in the corresponding sentence to be augmented\n",
        "augmentations = ['swap','delete']\n",
        "pct = [1, 1]\n",
        "aug_dict = dict(zip(augmentations, pct))\n",
        "\n",
        "# Fraction of the percentage\n",
        "frac_set = 3"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P720-Gfwf3he"
      },
      "source": [
        "aug_del = naw.RandomWordAug(action = augmentations[0], aug_p = aug_dict)\n",
        "aug_swap = naw.RandomWordAug(action=\"swap\", aug_p = 1)\n",
        "aug_index_min, aug_index_max = 0, len(sentences)//frac_set\n",
        "for augmentation in augmentations:\n",
        "  aug = naw.RandomWordAug(action = augmentation, aug_p = aug_dict[augmentation])\n",
        "  for i in range(aug_index_min, aug_index_max):  \n",
        "    augmented_text = aug.augment(sentences[i])  \n",
        "    augmented_sentences += [augmented_text]  \n",
        "  aug_index_min = aug_index_max\n",
        "  aug_index_max += min(len(sentences)-aug_index_max, len(sentences)//frac_set)\n",
        "  \n"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pzGQIxgzMYa9"
      },
      "source": [
        "### Backtranslation from Google Translate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PKXcKaZqBZPj",
        "outputId": "681d3d33-781b-4c5c-a33e-46bbeb8231bc"
      },
      "source": [
        "if aug_index_min < len(sentences):\n",
        "  translator = googletrans.Translator()\n",
        "  available_langs = list(googletrans.LANGUAGES.keys()) \n",
        "  trans_lang = random.choice(available_langs) \n",
        "  print(f\"Translating to {googletrans.LANGUAGES[trans_lang]}\")\n",
        "\n",
        "  translations = translator.translate(sentences[aug_index_min:], dest=trans_lang) \n",
        "  t_text = [t.text for t in translations]\n",
        "\n",
        "  translations_en_random = translator.translate(t_text, src=trans_lang, dest='en') \n",
        "  augmented_sentences += [t.text for t in translations_en_random]"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Translating to serbian\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2EUsLsODlZuq"
      },
      "source": [
        "df_train_aug = pd.DataFrame({'sentence': augmented_sentences, 'labels': [df_train['labels'][i] for i in indexes]})\n",
        "df_aug = pd.concat([df_train, df_train_aug]).reset_index(drop=True)\n",
        "df_aug = df_aug.sample(frac=1).reset_index(drop=True)"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "55c-wP_Go20c",
        "outputId": "319fd962-8113-440a-f3bd-9209100095ab"
      },
      "source": [
        "len(df_aug)"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8712"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Q3UIsHNtO7o"
      },
      "source": [
        "# Data Flow Utilities"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sxamu6H4Mj05"
      },
      "source": [
        "## Import the dependencies and configure reproducibility"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JDIyapAa6Pjr"
      },
      "source": [
        "import torch, torchtext\n",
        "from torchtext import data"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4kqZx2hG6e4Q"
      },
      "source": [
        "# Manual Seed\n",
        "SEED = 43\n",
        "torch.manual_seed(SEED)\n",
        "CUDA_LAUNCH_BLOCKING=1"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vOWb59V_M1wF"
      },
      "source": [
        "## Prepare the Train and Validation Datasets using the \"torchtext\" library"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wofFldA_tdYH"
      },
      "source": [
        "df_train = df_aug\n",
        "df_val = df_val"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aDBbe8Ad6yhE"
      },
      "source": [
        "df_val = df_val.reset_index(drop=True)"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "db3eiDVx6mKf"
      },
      "source": [
        "Sentence = torchtext.legacy.data.Field(sequential = True, tokenize = 'spacy', batch_first =True, include_lengths=True)\n",
        "Label = torchtext.legacy.data.LabelField(tokenize ='spacy', is_target=True, batch_first =True, sequential =False)"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-by1zHIV7LPI"
      },
      "source": [
        "fields = [('sentence', Sentence), ('label', Label)]"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GxmCFTgk797i"
      },
      "source": [
        "example_train = [torchtext.legacy.data.Example.fromlist([df_train.sentence[i],df_train.labels[i]], fields) for i in range(df_train.shape[0])] \n",
        "example_val = [torchtext.legacy.data.Example.fromlist([df_val.sentence[i],df_val.labels[i]], fields) for i in range(df_val.shape[0])] \n",
        "\n"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Lj9XCy38OqE"
      },
      "source": [
        "train = torchtext.legacy.data.Dataset(example_train, fields)\n",
        "valid = torchtext.legacy.data.Dataset(example_val, fields)"
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tsToO_PEu6zJ",
        "outputId": "b6ecf49f-af60-458a-b2aa-bf1bcda5f6ea"
      },
      "source": [
        "len(train), len(valid)"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(8712, 3557)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JX5_chR-FvqX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 203
        },
        "outputId": "d4dfc133-d0b9-40cf-c565-e6406b7b74b7"
      },
      "source": [
        "df_train.head()"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence</th>\n",
              "      <th>labels</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Donovan ... squanders his main asset , Jackie ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Quelle surprise !</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Seeks to transcend its genre with a curiously ...</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Metaphors abound , but it is easy to take this...</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Pan Nalin 's exposition is beautiful and myste...</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                            sentence  labels\n",
              "0  Donovan ... squanders his main asset , Jackie ...       1\n",
              "1                                  Quelle surprise !       2\n",
              "2  Seeks to transcend its genre with a curiously ...       3\n",
              "3  Metaphors abound , but it is easy to take this...       3\n",
              "4  Pan Nalin 's exposition is beautiful and myste...       3"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y8gXq6inM_lJ"
      },
      "source": [
        "## Build the vocab for the Train Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b_K23gxx84-K"
      },
      "source": [
        "Sentence.build_vocab(train)\n",
        "Label.build_vocab(train)"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zCPG8VrE9MKq",
        "outputId": "c7b1bda9-6d4d-4de5-e415-97c77061d576"
      },
      "source": [
        "print('Size of input vocab : ', len(Sentence.vocab))\n",
        "print('Size of label vocab : ', len(Label.vocab))\n",
        "print('Top 10 words appreared repeatedly :', list(Sentence.vocab.freqs.most_common(10)))\n",
        "print('Labels : ', Label.vocab.stoi)"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Size of input vocab :  16965\n",
            "Size of label vocab :  5\n",
            "Top 10 words appreared repeatedly : [('.', 8227), (',', 7299), ('the', 6211), ('of', 4478), ('a', 4454), ('and', 4431), ('to', 3105), ('-', 2873), ('is', 2590), (\"'s\", 2515)]\n",
            "Labels :  defaultdict(None, {3: 0, 1: 1, 2: 2, 4: 3, 0: 4})\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XCAJXiddNrV5"
      },
      "source": [
        "## Setting the Device for Process Handling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gBmMQQcX9SZk",
        "outputId": "17aa1ce9-e4d2-4c5e-c706-3f218245bf94"
      },
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lfIBEmIpN1NC"
      },
      "source": [
        "## Preparing DataLoaders for the Train and Validation Datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fIJyulXA9sEr"
      },
      "source": [
        "train_iterator, valid_iterator = torchtext.legacy.data.BucketIterator.splits((train, valid), batch_size = 16, \n",
        "                                                            sort_key = lambda x: len(x.sentence),\n",
        "                                                            sort_within_batch=True, device = device)"
      ],
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0kgqqK75FLEV",
        "outputId": "dc5757ac-c306-48a4-8f52-297398a80203"
      },
      "source": [
        "next(iter(train_iterator))\n",
        "#len(train.examples[11].tweet)"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\n",
              "[torchtext.legacy.data.batch.Batch of size 16]\n",
              "\t[.sentence]:('[torch.cuda.LongTensor of size 16x26 (GPU 0)]', '[torch.cuda.LongTensor of size 16 (GPU 0)]')\n",
              "\t[.label]:[torch.cuda.LongTensor of size 16 (GPU 0)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l_NVSpoV-Uaj"
      },
      "source": [
        "import os, pickle\n",
        "with open('tokenizer.pkl', 'wb') as tokens: \n",
        "    pickle.dump(Sentence.vocab.stoi, tokens)"
      ],
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a5py1Rwf3KlZ"
      },
      "source": [
        "# Building Model and defining Training and Validation Processes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mfo2rncyOf2t"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fNQnNcH6-oZZ"
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class classifier(nn.Module):\n",
        "    \n",
        "    # Define all the layers used in model\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, dropout):\n",
        "        \n",
        "        super().__init__()          \n",
        "        \n",
        "        # Embedding layer\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        \n",
        "        # LSTM layer\n",
        "        self.encoder = nn.LSTM(embedding_dim, \n",
        "                           hidden_dim, \n",
        "                           num_layers=n_layers, \n",
        "                           dropout=dropout,\n",
        "                           batch_first=True)\n",
        "        # try using nn.GRU or nn.RNN here and compare their performances\n",
        "        # try bidirectional and compare their performances\n",
        "        \n",
        "        # Dense layer\n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "        \n",
        "    def forward(self, text, text_lengths):\n",
        "        \n",
        "        # text = [batch size, sent_length]\n",
        "        embedded = self.embedding(text)\n",
        "        # embedded = [batch size, sent_len, emb dim]\n",
        "      \n",
        "        # packed sequence\n",
        "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, text_lengths.cpu(), batch_first=True)\n",
        "        \n",
        "        packed_output, (hidden, cell) = self.encoder(packed_embedded)\n",
        "        #hidden = [batch size, num layers * num directions,hid dim]\n",
        "        #cell = [batch size, num layers * num directions,hid dim]\n",
        "    \n",
        "        # Hidden = [batch size, hid dim * num directions]\n",
        "        dense_outputs = self.fc(hidden)   \n",
        "        \n",
        "        # Final activation function softmax\n",
        "        #output = F.softmax(dense_outputs[0], dim=1)\n",
        "            \n",
        "        return dense_outputs[0]"
      ],
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TFNWimMMAKya"
      },
      "source": [
        "# Define hyperparameters\n",
        "size_of_vocab = len(Sentence.vocab)\n",
        "embedding_dim = 300\n",
        "num_hidden_nodes = 100\n",
        "num_output_nodes = 5\n",
        "num_layers = 2\n",
        "dropout = 0.2\n",
        "\n",
        "# Instantiate the model\n",
        "model = classifier(size_of_vocab, embedding_dim, num_hidden_nodes, num_output_nodes, num_layers, dropout = dropout)"
      ],
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IRextCcAASGO",
        "outputId": "f3e8c30e-63f7-4896-e4f5-465ce9f9c2d2"
      },
      "source": [
        "print(model)\n",
        "\n",
        "#No. of trianable parameters\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "    \n",
        "print(f'The model has {count_parameters(model):,} trainable parameters')"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "classifier(\n",
            "  (embedding): Embedding(16965, 300)\n",
            "  (encoder): LSTM(300, 100, num_layers=2, batch_first=True, dropout=0.2)\n",
            "  (fc): Linear(in_features=100, out_features=5, bias=True)\n",
            ")\n",
            "The model has 5,331,605 trainable parameters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EPK6b19HATLm"
      },
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "# define optimizer and loss\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-5)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# define metric\n",
        "def binary_accuracy(preds, y):\n",
        "    #round predictions to the closest integer\n",
        "    _, predictions = torch.max(preds, 1)\n",
        "    \n",
        "    correct = (predictions == y).float() \n",
        "    acc = correct.sum() / len(correct)\n",
        "    return acc\n",
        "    \n",
        "# push to cuda if available\n",
        "model = model.to(device)\n",
        "criterion = criterion.to(device)"
      ],
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2RsjF5wjOlSq"
      },
      "source": [
        "## Training Process"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A8t9iWwqAify"
      },
      "source": [
        "def train(model, iterator, optimizer, criterion):\n",
        "    \n",
        "    # initialize every epoch \n",
        "    \n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    \n",
        "    # set the model in training phase\n",
        "    model.train()  \n",
        "    \n",
        "    for batch in iterator:\n",
        "        # resets the gradients after every batch\n",
        "        optimizer.zero_grad() \n",
        "        \n",
        "        \n",
        "        # retrieve text and no. of words\n",
        "        sentence, sentence_lengths = batch.sentence  \n",
        "        \n",
        "        # convert to 1D tensor\n",
        "        predictions = model(sentence, sentence_lengths).squeeze()  \n",
        "        #print(predictions, batch.label)\n",
        "        # compute the loss\n",
        "        loss = criterion(predictions, batch.label)        \n",
        "        \n",
        "        # compute the binary accuracy\n",
        "        acc = binary_accuracy(predictions, batch.label)   \n",
        "        \n",
        "        # backpropage the loss and compute the gradients\n",
        "        loss.backward()       \n",
        "        \n",
        "        # update the weights\n",
        "        optimizer.step()      \n",
        "        \n",
        "        # loss and accuracy\n",
        "        epoch_loss += loss.item()  \n",
        "        epoch_acc += acc.item()    \n",
        "        \n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
      ],
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VOgesstfOn3C"
      },
      "source": [
        "## Validation Process"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LMBXHd5JAuX-"
      },
      "source": [
        "def evaluate(model, iterator, criterion):\n",
        "    \n",
        "    # initialize every epoch\n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "\n",
        "    # deactivating dropout layers\n",
        "    model.eval()\n",
        "    \n",
        "    # deactivates autograd\n",
        "    with torch.no_grad():\n",
        "    \n",
        "        for batch in iterator:\n",
        "        \n",
        "            # retrieve text and no. of words\n",
        "            sentence, sentence_lengths = batch.sentence\n",
        "            \n",
        "            # convert to 1d tensor\n",
        "            predictions = model(sentence, sentence_lengths).squeeze() \n",
        "            \n",
        "            # compute loss and accuracy\n",
        "            loss = criterion(predictions, batch.label)\n",
        "            acc = binary_accuracy(predictions, batch.label)\n",
        "            \n",
        "            # keep track of loss and accuracy\n",
        "            epoch_loss += loss.item()\n",
        "            epoch_acc += acc.item()\n",
        "        \n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
      ],
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tia78tuJO0Ux"
      },
      "source": [
        "## Main Process - Training and Validating for a certain number of epochs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q7UPwN0KAvVq",
        "outputId": "80335d51-8b46-4d81-d48e-1b41033747b0"
      },
      "source": [
        "N_EPOCHS = 50\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "     \n",
        "    # train the model\n",
        "    train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n",
        "    \n",
        "    # evaluate the model\n",
        "    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)\n",
        "    \n",
        "    # save the best model\n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model.state_dict(), 'saved_weights.pt')\n",
        "    \n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
        "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}% \\n')"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\tTrain Loss: 1.586 | Train Acc: 25.45%\n",
            "\t Val. Loss: 1.578 |  Val. Acc: 25.26% \n",
            "\n",
            "\tTrain Loss: 1.572 | Train Acc: 26.81%\n",
            "\t Val. Loss: 1.574 |  Val. Acc: 25.99% \n",
            "\n",
            "\tTrain Loss: 1.570 | Train Acc: 27.11%\n",
            "\t Val. Loss: 1.573 |  Val. Acc: 25.99% \n",
            "\n",
            "\tTrain Loss: 1.567 | Train Acc: 28.08%\n",
            "\t Val. Loss: 1.572 |  Val. Acc: 26.55% \n",
            "\n",
            "\tTrain Loss: 1.565 | Train Acc: 28.66%\n",
            "\t Val. Loss: 1.571 |  Val. Acc: 26.16% \n",
            "\n",
            "\tTrain Loss: 1.563 | Train Acc: 28.96%\n",
            "\t Val. Loss: 1.571 |  Val. Acc: 26.41% \n",
            "\n",
            "\tTrain Loss: 1.561 | Train Acc: 29.53%\n",
            "\t Val. Loss: 1.569 |  Val. Acc: 27.31% \n",
            "\n",
            "\tTrain Loss: 1.559 | Train Acc: 30.06%\n",
            "\t Val. Loss: 1.568 |  Val. Acc: 27.56% \n",
            "\n",
            "\tTrain Loss: 1.557 | Train Acc: 30.39%\n",
            "\t Val. Loss: 1.568 |  Val. Acc: 27.48% \n",
            "\n",
            "\tTrain Loss: 1.555 | Train Acc: 30.40%\n",
            "\t Val. Loss: 1.566 |  Val. Acc: 27.81% \n",
            "\n",
            "\tTrain Loss: 1.552 | Train Acc: 31.36%\n",
            "\t Val. Loss: 1.565 |  Val. Acc: 27.61% \n",
            "\n",
            "\tTrain Loss: 1.549 | Train Acc: 31.54%\n",
            "\t Val. Loss: 1.564 |  Val. Acc: 27.58% \n",
            "\n",
            "\tTrain Loss: 1.546 | Train Acc: 31.61%\n",
            "\t Val. Loss: 1.563 |  Val. Acc: 28.09% \n",
            "\n",
            "\tTrain Loss: 1.543 | Train Acc: 32.00%\n",
            "\t Val. Loss: 1.561 |  Val. Acc: 28.51% \n",
            "\n",
            "\tTrain Loss: 1.540 | Train Acc: 32.49%\n",
            "\t Val. Loss: 1.560 |  Val. Acc: 28.12% \n",
            "\n",
            "\tTrain Loss: 1.536 | Train Acc: 32.57%\n",
            "\t Val. Loss: 1.559 |  Val. Acc: 28.09% \n",
            "\n",
            "\tTrain Loss: 1.532 | Train Acc: 32.95%\n",
            "\t Val. Loss: 1.558 |  Val. Acc: 28.09% \n",
            "\n",
            "\tTrain Loss: 1.529 | Train Acc: 33.26%\n",
            "\t Val. Loss: 1.556 |  Val. Acc: 29.24% \n",
            "\n",
            "\tTrain Loss: 1.524 | Train Acc: 33.56%\n",
            "\t Val. Loss: 1.555 |  Val. Acc: 29.41% \n",
            "\n",
            "\tTrain Loss: 1.520 | Train Acc: 33.88%\n",
            "\t Val. Loss: 1.553 |  Val. Acc: 29.46% \n",
            "\n",
            "\tTrain Loss: 1.515 | Train Acc: 34.21%\n",
            "\t Val. Loss: 1.552 |  Val. Acc: 29.38% \n",
            "\n",
            "\tTrain Loss: 1.511 | Train Acc: 34.42%\n",
            "\t Val. Loss: 1.550 |  Val. Acc: 29.61% \n",
            "\n",
            "\tTrain Loss: 1.506 | Train Acc: 34.89%\n",
            "\t Val. Loss: 1.549 |  Val. Acc: 29.89% \n",
            "\n",
            "\tTrain Loss: 1.501 | Train Acc: 35.11%\n",
            "\t Val. Loss: 1.548 |  Val. Acc: 30.17% \n",
            "\n",
            "\tTrain Loss: 1.495 | Train Acc: 35.64%\n",
            "\t Val. Loss: 1.546 |  Val. Acc: 30.25% \n",
            "\n",
            "\tTrain Loss: 1.490 | Train Acc: 36.12%\n",
            "\t Val. Loss: 1.545 |  Val. Acc: 30.48% \n",
            "\n",
            "\tTrain Loss: 1.484 | Train Acc: 36.40%\n",
            "\t Val. Loss: 1.543 |  Val. Acc: 30.81% \n",
            "\n",
            "\tTrain Loss: 1.478 | Train Acc: 36.80%\n",
            "\t Val. Loss: 1.542 |  Val. Acc: 30.87% \n",
            "\n",
            "\tTrain Loss: 1.471 | Train Acc: 37.12%\n",
            "\t Val. Loss: 1.540 |  Val. Acc: 31.12% \n",
            "\n",
            "\tTrain Loss: 1.465 | Train Acc: 37.52%\n",
            "\t Val. Loss: 1.538 |  Val. Acc: 31.12% \n",
            "\n",
            "\tTrain Loss: 1.458 | Train Acc: 38.17%\n",
            "\t Val. Loss: 1.536 |  Val. Acc: 31.46% \n",
            "\n",
            "\tTrain Loss: 1.451 | Train Acc: 38.33%\n",
            "\t Val. Loss: 1.535 |  Val. Acc: 32.02% \n",
            "\n",
            "\tTrain Loss: 1.443 | Train Acc: 39.01%\n",
            "\t Val. Loss: 1.533 |  Val. Acc: 31.96% \n",
            "\n",
            "\tTrain Loss: 1.435 | Train Acc: 39.75%\n",
            "\t Val. Loss: 1.530 |  Val. Acc: 32.13% \n",
            "\n",
            "\tTrain Loss: 1.427 | Train Acc: 40.14%\n",
            "\t Val. Loss: 1.528 |  Val. Acc: 32.13% \n",
            "\n",
            "\tTrain Loss: 1.418 | Train Acc: 40.60%\n",
            "\t Val. Loss: 1.526 |  Val. Acc: 32.83% \n",
            "\n",
            "\tTrain Loss: 1.408 | Train Acc: 41.38%\n",
            "\t Val. Loss: 1.523 |  Val. Acc: 32.84% \n",
            "\n",
            "\tTrain Loss: 1.398 | Train Acc: 41.86%\n",
            "\t Val. Loss: 1.521 |  Val. Acc: 32.56% \n",
            "\n",
            "\tTrain Loss: 1.388 | Train Acc: 42.47%\n",
            "\t Val. Loss: 1.517 |  Val. Acc: 33.20% \n",
            "\n",
            "\tTrain Loss: 1.377 | Train Acc: 42.91%\n",
            "\t Val. Loss: 1.513 |  Val. Acc: 33.62% \n",
            "\n",
            "\tTrain Loss: 1.365 | Train Acc: 43.59%\n",
            "\t Val. Loss: 1.510 |  Val. Acc: 33.64% \n",
            "\n",
            "\tTrain Loss: 1.353 | Train Acc: 43.99%\n",
            "\t Val. Loss: 1.506 |  Val. Acc: 33.92% \n",
            "\n",
            "\tTrain Loss: 1.341 | Train Acc: 44.85%\n",
            "\t Val. Loss: 1.502 |  Val. Acc: 34.06% \n",
            "\n",
            "\tTrain Loss: 1.329 | Train Acc: 45.02%\n",
            "\t Val. Loss: 1.501 |  Val. Acc: 34.10% \n",
            "\n",
            "\tTrain Loss: 1.317 | Train Acc: 45.92%\n",
            "\t Val. Loss: 1.497 |  Val. Acc: 34.46% \n",
            "\n",
            "\tTrain Loss: 1.306 | Train Acc: 46.35%\n",
            "\t Val. Loss: 1.495 |  Val. Acc: 34.63% \n",
            "\n",
            "\tTrain Loss: 1.295 | Train Acc: 46.65%\n",
            "\t Val. Loss: 1.493 |  Val. Acc: 34.57% \n",
            "\n",
            "\tTrain Loss: 1.284 | Train Acc: 47.22%\n",
            "\t Val. Loss: 1.491 |  Val. Acc: 34.46% \n",
            "\n",
            "\tTrain Loss: 1.273 | Train Acc: 47.99%\n",
            "\t Val. Loss: 1.489 |  Val. Acc: 34.74% \n",
            "\n",
            "\tTrain Loss: 1.263 | Train Acc: 48.28%\n",
            "\t Val. Loss: 1.490 |  Val. Acc: 34.69% \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZoJLxQXzQpvj"
      },
      "source": [
        "# Evaluating the Model on Random Inputs from the Validation Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BXCGxks4AxT3"
      },
      "source": [
        "#load weights and tokenizer\n",
        "\n",
        "path='./saved_weights.pt'\n",
        "model.load_state_dict(torch.load(path));\n",
        "model.eval();\n",
        "tokenizer_file = open('./tokenizer.pkl', 'rb')\n",
        "tokenizer = pickle.load(tokenizer_file)\n",
        "\n",
        "#inference \n",
        "\n",
        "import spacy\n",
        "nlp = spacy.load('en')\n",
        "\n",
        "def classify_sentence(sentence):\n",
        "    \n",
        "    categories = {0: \"Very Negative\", 1:\"Negative\", 2:\"Neutral\", 3:\"Positive\", 4:\"Very Positive\"}\n",
        "    \n",
        "    # tokenize the tweet \n",
        "    tokenized = [tok.text for tok in nlp.tokenizer(sentence)] \n",
        "    # convert to integer sequence using predefined tokenizer dictionary\n",
        "    indexed = [tokenizer[t] for t in tokenized]        \n",
        "    # compute no. of words        \n",
        "    length = [len(indexed)]\n",
        "    # convert to tensor                                    \n",
        "    tensor = torch.LongTensor(indexed).to(device)   \n",
        "    # reshape in form of batch, no. of words           \n",
        "    tensor = tensor.unsqueeze(1).T  \n",
        "    # convert to tensor                          \n",
        "    length_tensor = torch.LongTensor(length)\n",
        "    # Get the model prediction                  \n",
        "    prediction = model(tensor, length_tensor)\n",
        "\n",
        "    _, pred = torch.max(prediction, 1) \n",
        "    \n",
        "    return categories[pred.item()]"
      ],
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nox24SPjKS6N"
      },
      "source": [
        "# random sentences from validation set for the model to be tested upon\n",
        "num_val = 10"
      ],
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zo7c72BuH54w",
        "outputId": "912bea0a-b75c-4a4a-a8db-8ffbc3379a02"
      },
      "source": [
        "categories = {0: \"Very Negative\", 1:\"Negative\", 2:\"Neutral\", 3:\"Positive\", 4:\"Very Positive\"}\n",
        "evaluation_sentence_indices = random.sample(range(0, len(df_val)), num_val)\n",
        "for i in range(num_val):\n",
        "  print(f\"Actual Sentence : {df_val['sentence'][evaluation_sentence_indices[i]]} \\nActual Sentiment: {categories[df_val['labels'][evaluation_sentence_indices[i]]]}\")\n",
        "  print(f\"Predicted Sentiment: {classify_sentence(df_val['sentence'][evaluation_sentence_indices[i]])}\")\n",
        "  print(\"----------------------------------------\")"
      ],
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Actual Sentence : The setting turns out to be more interesting than any of the character dramas , which never reach satisfying conclusions . \n",
            "Actual Sentiment: Negative\n",
            "Predicted Sentiment: Negative\n",
            "----------------------------------------\n",
            "Actual Sentence : A culture clash comedy only half as clever as it thinks it is . \n",
            "Actual Sentiment: Negative\n",
            "Predicted Sentiment: Negative\n",
            "----------------------------------------\n",
            "Actual Sentence : Formula 51 has dulled your senses faster and deeper than any recreational drug on the market . \n",
            "Actual Sentiment: Neutral\n",
            "Predicted Sentiment: Negative\n",
            "----------------------------------------\n",
            "Actual Sentence : The movie , while beautiful , feels labored , with a hint of the writing exercise about it . \n",
            "Actual Sentiment: Neutral\n",
            "Predicted Sentiment: Very Negative\n",
            "----------------------------------------\n",
            "Actual Sentence : This is art paying homage to art . \n",
            "Actual Sentiment: Positive\n",
            "Predicted Sentiment: Very Negative\n",
            "----------------------------------------\n",
            "Actual Sentence : No one can doubt the filmmakers ' motives , but The Guys still feels counterproductive . \n",
            "Actual Sentiment: Negative\n",
            "Predicted Sentiment: Negative\n",
            "----------------------------------------\n",
            "Actual Sentence : Kinnear 's performance is a career-defining revelation . \n",
            "Actual Sentiment: Very Positive\n",
            "Predicted Sentiment: Very Negative\n",
            "----------------------------------------\n",
            "Actual Sentence : For a film that 's being advertised as a comedy , Sweet Home Alabama is n't as funny as you 'd hoped . \n",
            "Actual Sentiment: Negative\n",
            "Predicted Sentiment: Negative\n",
            "----------------------------------------\n",
            "Actual Sentence : A movie that seems motivated more by a desire to match mortarboards with Dead Poets Society and Good Will Hunting than by its own story . \n",
            "Actual Sentiment: Neutral\n",
            "Predicted Sentiment: Negative\n",
            "----------------------------------------\n",
            "Actual Sentence : The fact that the ` best part ' of the movie comes from a 60-second homage to one of Demme 's good films does n't bode well for the rest of it . \n",
            "Actual Sentiment: Negative\n",
            "Predicted Sentiment: Very Negative\n",
            "----------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}
