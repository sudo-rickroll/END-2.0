# PyTorch Attention

This directory contains the notebook to illustrate attention in NLP on Seq2Seq models, as implemented by PyTorch. This implementation is done on the English-French translation pairs from the dataset hosted <a href="https://download.pytorch.org/tutorial/data.zip">here</a>. English sentences in the dataset are considered to be the sources and their corresponding French sentence counterparts are considered the target.
